{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global imports\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "# Please install langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_data = pd.pandas.read_csv('data/en.openfoodfacts.org.products.csv', sep='\\t', parse_dates=True, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a lot of NaN values in the dataset\n",
    "food_facts_data.isnull().mean().sort_values(ascending=True).plot.barh(figsize = (8, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First idea\n",
    "We are very curious in \"Food carbon footprint\": greenhouse gas emissions produced by growing, rearing, farming, processing, transporting, storing, cooking and disposing of the food we eat. We are interested in finding out the countries, which are emitting huge amount of carbon in the result of food production and consumption, and identify the ways to reduce it. We hope the dataset will give us insights about the products and their origins having highest carbon footprint(i.e. meat, cheese,eggs, so on) and the ones having lower carbon footprints(i.e. fruit, vegetables, beans, nuts so on). Additionally we consider that food packaging and food waste treatment have huge proportion of impact on carbon emission, thus these were hypothesis that we were interested to test and make conclusions about global problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARBON_FOOD_PRINT_MASK = ~food_facts_data['carbon-footprint_100g'].isnull()\n",
    "print(\"Number of food samples with valid carbon foot print is {}\".format(food_facts_data[CARBON_FOOD_PRINT_MASK].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Carbon-foot-print.\n",
    "food_facts_data['carbon-footprint_100g'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = food_facts_data.dtypes[food_facts_data.dtypes ==  'float64'].index\n",
    "\n",
    "# Plotting the correlation matrix for carbon food print\n",
    "corr_ = food_facts_data[CARBON_FOOD_PRINT_MASK][numeric_columns].corr().abs()\n",
    "fig, axis = plt.subplots(figsize=(10, 8))\n",
    "sn.heatmap(corr_, mask=np.zeros_like(corr_, dtype=np.bool), cmap=sn.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort the features having highest correlation with carbon footprint\n",
    "valid_corr_columns = corr_['carbon-footprint_100g'].dropna().sort_values(ascending=False)\n",
    "valid_corr_columns = valid_corr_columns[valid_corr_columns > 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out number of data samples with high-correlation with other non-numeric columns.\n",
    "food_facts_data[food_facts_data['carbon-footprint_100g'].notnull()][valid_corr_columns.index].notnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Unfortunately the high correlation is because of the lack of datapoints, it has been shown that the proportions of NAN is very high in the dataset where the carbon footprint has some information. So our thoughts that we may apply regression and other fill in tools to fill the carbon footprint values are ruined, we decided to leave this idea because we dont have enough data and didn't find the carbon footprint of each food product to make real conculsions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch to the second idea\n",
    " Our final idea is  \"Food allergy\", which happens when human body overreact to exposure to particular substances in the food.\n",
    "Our aim in this idea is to identify the possible allergies that unlabeled food may cause and recommend the foods that has specific nutrition  and aren't affecting to people with specific allergies.\n",
    "The social goodness will be to help food allergic people find their healthy and enough proportion of  food easily.\n",
    "\n",
    "\n",
    "So we will mostly observe the Additives, Ingredients, Nutrients, Allergens and preprocess them for our next milestone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additives and Nutrients analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distribution of Additives in the dataset\n",
    "number_additives = food_facts_data[food_facts_data.additives_en.notnull()].additives_en.str.split(',').values.flatten()\n",
    "number_additives = np.concatenate(number_additives)\n",
    "number_additives = np.unique(number_additives, return_counts=True)\n",
    "number_additives = pd.Series(number_additives[1], index=number_additives[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_additives.sort_values()[-20:].plot.barh()\n",
    "plt.title(\"The top additives in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the number of unique additives in the allergens dataset is {}\".format(number_additives.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation of dataset each food item is assigned a unique code. So we can make code as the index of our data frame and relate all the other datasets on this index.\n",
    "\n",
    "However, we find that the code is not unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    food_facts_data.set_index('code', inplace=True, drop=False)\n",
    "except:\n",
    "    print('code is already index')\n",
    "    \n",
    "print(\"Is the dataset index unique? {}\".format(food_facts_data.index.is_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets investigate the data samples with the same bar code.\n",
    "_not_unique = food_facts_data[['code']].groupby(food_facts_data.index).count() != 1\n",
    "food_facts_data.loc[_not_unique[_not_unique.code].index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data contains duplicate values for all columns, except for last_modified time. Since the food product is same we pick the latest samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_data.drop(columns=['code'], inplace=True)\n",
    "food_facts_data = food_facts_data.groupby(food_facts_data.index).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is index unique: {}\".format(food_facts_data.index.is_unique))\n",
    "print(\"Number of na index: {}\".format(food_facts_data.index.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the na code food item as the data is corrupted\n",
    "food_facts_data = food_facts_data.drop(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of na index: {}\".format(food_facts_data.index.isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first analyse the nutritional facts of all the available products. So we keep columns 'product_name', 'categories'\n",
    "and all nutrition features (number of features: 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTRIENTS_ALL = [\n",
    "       'energy_100g', 'energy-from-fat_100g', 'fat_100g',\n",
    "       'saturated-fat_100g', '-butyric-acid_100g', '-caproic-acid_100g',\n",
    "       '-caprylic-acid_100g', '-capric-acid_100g', '-lauric-acid_100g',\n",
    "       '-myristic-acid_100g', '-palmitic-acid_100g', '-stearic-acid_100g',\n",
    "       '-arachidic-acid_100g', '-behenic-acid_100g',\n",
    "       '-lignoceric-acid_100g', '-cerotic-acid_100g',\n",
    "       '-montanic-acid_100g', '-melissic-acid_100g',\n",
    "       'monounsaturated-fat_100g', 'polyunsaturated-fat_100g',\n",
    "       'omega-3-fat_100g', '-alpha-linolenic-acid_100g',\n",
    "       '-eicosapentaenoic-acid_100g', '-docosahexaenoic-acid_100g',\n",
    "       'omega-6-fat_100g', '-linoleic-acid_100g',\n",
    "       '-arachidonic-acid_100g', '-gamma-linolenic-acid_100g',\n",
    "       '-dihomo-gamma-linolenic-acid_100g', 'omega-9-fat_100g',\n",
    "       '-oleic-acid_100g', '-elaidic-acid_100g', '-gondoic-acid_100g',\n",
    "       '-mead-acid_100g', '-erucic-acid_100g', '-nervonic-acid_100g',\n",
    "       'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g',\n",
    "       'sugars_100g', '-sucrose_100g', '-glucose_100g', '-fructose_100g',\n",
    "       '-lactose_100g', '-maltose_100g', '-maltodextrins_100g',\n",
    "       'starch_100g', 'polyols_100g', 'fiber_100g', 'proteins_100g',\n",
    "       'casein_100g', 'serum-proteins_100g', 'nucleotides_100g',\n",
    "       'salt_100g', 'sodium_100g', 'alcohol_100g', 'vitamin-a_100g',\n",
    "       'beta-carotene_100g', 'vitamin-d_100g', 'vitamin-e_100g',\n",
    "       'vitamin-k_100g', 'vitamin-c_100g', 'vitamin-b1_100g',\n",
    "       'vitamin-b2_100g', 'vitamin-pp_100g', 'vitamin-b6_100g',\n",
    "       'vitamin-b9_100g', 'folates_100g', 'vitamin-b12_100g',\n",
    "       'biotin_100g', 'pantothenic-acid_100g', 'silica_100g',\n",
    "       'bicarbonate_100g', 'potassium_100g', 'chloride_100g',\n",
    "       'calcium_100g', 'phosphorus_100g', 'iron_100g', 'magnesium_100g',\n",
    "       'zinc_100g', 'copper_100g', 'manganese_100g', 'fluoride_100g',\n",
    "       'selenium_100g', 'chromium_100g', 'molybdenum_100g', 'iodine_100g',\n",
    "       'caffeine_100g', 'taurine_100g', 'ph_100g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts = food_facts_data[NUTRIENTS_ALL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that some nutrients have only very few data points. (For example -butyric-acid has only one data point)\n",
    "So, we keep only top KEEP_NUTRITION nutrients at the moment so that count is greater than 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nutrition = nutrition_facts.count().sort_values(ascending=False)\n",
    "count_nutrition = count_nutrition[count_nutrition>1000.]\n",
    "count_nutrition.plot.barh(figsize=(16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTRIENTS_FEATURES = count_nutrition.index.values.tolist()\n",
    "# Now we have 36 nutrients columns.\n",
    "# 'energy_100g', 'proteins_100g', 'fat_100g', 'carbohydrates_100g', 'sugars_100g', 'salt_100g', 'sodium_100g',\n",
    "# 'saturated-fat_100g', 'fiber_100g', 'cholesterol_100g', 'trans-fat_100g', 'calcium_100g', 'vitamin-c_100g', 'iron_100g',\n",
    "# 'vitamin-a_100g', 'potassium_100g', 'polyunsaturated-fat_100g', 'monounsaturated-fat_100g', 'vitamin-pp_100g', \n",
    "# 'vitamin-b1_100g', 'vitamin-b2_100g', 'alcohol_100g', 'vitamin-d_100g', 'vitamin-b6_100g', 'magnesium_100g', 'phosphorus_100g',\n",
    "# 'vitamin-b12_100g', 'vitamin-b9_100g', 'zinc_100g', 'folates_100g', 'pantothenic-acid_100g', 'copper_100g', 'vitamin-e_100g',\n",
    "# 'manganese_100g', 'selenium_100g', 'omega-3-fat_100g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts = nutrition_facts[NUTRIENTS_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also observe many outliers in the dataset. For example, for some products proteins_100g value is 31000 which should not be\n",
    "possible as we are measuring the quantities in per 100 g. One possible explaination would be that the units of measurements\n",
    "are not consistent with other samples. Also, some nutrients values are negative. These values may be as a result of error in\n",
    "interpreting - sign as negative.\n",
    "Presently, we remove these outliers from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    # First remove the negative sign\n",
    "    df = df.abs()\n",
    "    \n",
    "    # As the weight of the food item is 100g, the weight of nutrients can never exceed 100g.\n",
    "    threshold_cutoff = pd.Series(100*np.ones(df.columns.size), index=df.columns)\n",
    "    \n",
    "    # Since energy_100g measured in kilo joules(kJ) we don't clip it. However the highest energy is given by fats (37kJ/g).\n",
    "    # So the upper bound of energy per 100g should be 3700kJ.\n",
    "    threshold_cutoff.loc['energy_100g'] = 40*1e2\n",
    "    \n",
    "    return df[df <= threshold_cutoff].dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_filt = remove_outliers(nutrition_facts)\n",
    "print(\"Nutrition data shape {}\".format(nutrition_facts_filt.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_filt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions_nutrition(df, nlog_scale_col):\n",
    "    fig, axes = plt.subplots(9, len(df.columns)//9, figsize=(30, 30))\n",
    "    axes = axes.flatten()\n",
    "    for col, axis in zip(df.columns, axes):\n",
    "        bins = np.linspace(df[col].min(), df[col].quantile(0.99), 50)\n",
    "        df.hist(column = col, bins = bins, ax=axis)\n",
    "        if not col in nlog_scale_col:\n",
    "            axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions_nutrition(nutrition_facts_filt, nlog_scale_col=['energy_100g', 'proteins_100g', 'fat_100g',\n",
    "                                                                  'carbohydrates_100g', 'sugars_100g', \n",
    "                                                                   'omega-3-fat_100g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a nutrition value is not mentioned on the label (NaN), we can assume that the nutrient is absent in the food item.\n",
    "Thus we replace all NaN with 0. This is because in the absence of any nutrition value, we have no way to know if the product\n",
    "actually has that nutrient. For the outlier products, our proposed solution of recommending alternate food product\n",
    "based on nutrients (for allergies and additives) can give wrong results. But we see that this problem even arises with OpenFood\n",
    "website that calcultes nutri-score wrong if outliers are present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_values(df):\n",
    "    df[df.columns.drop('energy_100g')] = df[df.columns.drop('energy_100g')].replace(np.NaN, 0)\n",
    "    return df\n",
    "\n",
    "nutrition_facts_filt = remove_nan_values(nutrition_facts_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of nutrition data after missing values removal {}\".format(nutrition_facts_filt[nutrition_facts_filt.energy_100g.isna()].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Observe that there are 6424 products with unknown energy_100g. \n",
    "So, for these products we can calculate approximate energy.\n",
    "\n",
    "\n",
    "$${energy{100g}} = 17*{protein{100g}} + 37*{fat{100g}} + 17*{carbohydrate{100g}} + 8*{fibre{100g}} + 29*{alcohol_{100g}}$$\n",
    "\n",
    "\n",
    "source: http://www.mydailyintake.net/energy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_energy_nan(df):\n",
    "    na_mask = df['energy_100g'].isna()\n",
    "    df.loc[df.index[na_mask], 'energy_100g'] = \\\n",
    "                        17*(df[na_mask].proteins_100g + df[na_mask].carbohydrates_100g) +\\\n",
    "                        37*df[na_mask].fat_100g + 8*df[na_mask].fiber_100g + 29*df[na_mask].alcohol_100g\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_filt = remove_energy_nan(nutrition_facts_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_data.drop(NUTRIENTS_ALL, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all the changes to the original data\n",
    "food_facts_data = food_facts_data.merge(nutrition_facts_filt, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do some research on how ingredients text and allergens are like and what transformation do we need to perform for each one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the data where ingredients text and allergens are applicable\n",
    "food_allergens_mask = food_facts_data[\"ingredients_text\"].notnull() & food_facts_data[\"allergens\"].notnull()\n",
    "food_allergens = food_facts_data[food_allergens_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of products is: {}\".format(food_facts_data.shape[0]))\n",
    "print(\"The total number of products with food allergens ando ingredients data is: {}\".format(food_allergens.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****This is approximately 10% of the dataset and hence we have more training data then carbon footprint****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the language for a particular food / allergen product - as to which language is it written in\n",
    "def detect_language(phrase):\n",
    "    try:        \n",
    "        if not phrase:\n",
    "            return 'No language'\n",
    "        \n",
    "        language_id = detect(phrase)\n",
    "        return language_id\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens['language_id'] = food_allergens[\"ingredients_text\"][:2000].apply(lambda x: detect_language(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we take the first 2000 samples to see how many languages are there\n",
    "As, we can see there are 22 languages for ingredients_text which is a ***very difficult translation task in its own!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens.groupby('language_id').count()['url'].plot.barh()\n",
    "plt.title(\"The frequency of used Languages\")\n",
    "food_allergens = food_allergens.drop(columns=[\"language_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now trying to find the number of languages present in the allergens column\n",
    "food_allergens['language_id'] = food_allergens[\"allergens\"][:2000].apply(lambda x: detect_language(x))\n",
    "food_allergens.groupby('language_id').count()['url'].plot.barh()\n",
    "plt.title(\"the number of languages present in the allergens column\")\n",
    "food_allergens = food_allergens.drop(columns=[\"language_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allergens_dict = {}\n",
    "with open(\"original_allergens.txt\") as f, open(\"english_allergens.txt\") as f2:\n",
    "    for x, y in zip(f, f2):\n",
    "        allergens_dict[x.lower().replace('\\n', '').strip().rstrip()] = re.sub(' +', ' ', y).lower().replace('\\n', '').strip().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_to_list(phrase, match_dict=None):\n",
    "    # Sometimes the word has a incorrect language tag in front of it\n",
    "    # So we are removing all tags and using language detection instead\n",
    "    foods = []\n",
    "\n",
    "    for food in phrase.split(','):\n",
    "        food = food.strip().rstrip()\n",
    "        \n",
    "        # Delete all additives since additives are nearly defined for the whole dataset\n",
    "        food = re.sub(' +', ' ', food).lower()\n",
    "        food = re.sub(r\"[^A-Za-zàâçéèêëîïôûùüÿñæœ\\'\\s,]\", '', food)\n",
    "        \n",
    "        # or food items which are made up of one characters\n",
    "        if(len(food) <= 2):\n",
    "            continue\n",
    "        \n",
    "        # Split the tag - en, fr if they have it\n",
    "        formatted_food = food.split(':')[1] if ':' in food else food\n",
    "    \n",
    "        if match_dict and formatted_food in match_dict:\n",
    "            translated_food = match_dict[formatted_food]\n",
    "        else:\n",
    "            translated_food = formatted_food\n",
    "        foods.append(translated_food)\n",
    "    return foods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transferring all french allergens or allergens in any other language to the english language from the translated files we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens['allergens'] = food_allergens['allergens'].apply((lambda x: split_string_to_list(x, match_dict=allergens_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens['allergens'] = food_allergens['allergens'].apply(frozenset) # Removing all the duplicates from each of the product\n",
    "unique_allergens = frozenset.union(*food_allergens['allergens'])\n",
    "\n",
    "print(\"The total number of food allergens before cleaning them for less than 5 products: {}\".format(len(unique_allergens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we get the allergy count for each of the allergies which are present in more than 5 products\n",
    "allergen_count = {}\n",
    "temp_allergens = list(unique_allergens)\n",
    "\n",
    "# Calculating the allergens and removing the allergens which are present in less than 5 products\n",
    "for allergen in unique_allergens:\n",
    "    count = food_allergens['allergens'].apply(lambda x: int(allergen in x)).sum()\n",
    "    \n",
    "    # Remove allergen if it is present in less than 5 products\n",
    "    if count < 5:\n",
    "        temp_allergens.remove(allergen)\n",
    "        continue\n",
    "    allergen_count[allergen] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(allergen_count).sort_values()[-20:].plot.barh() \n",
    "plt.title(\"top most 20 allergens in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of allergens after cleaning them by considering them for greater than 5 products: {}'.format(\n",
    "    len(allergen_count.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the intersection of all allergens which coexist with \n",
    "allergen_set = frozenset(allergen_count.keys())\n",
    "food_allergens['allergens'] = food_allergens['allergens'].apply(lambda x: allergen_set.intersection(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_allergens_df = pd.DataFrame(pd.np.empty((food_allergens.shape[0],len(allergen_set))) * 0, columns=allergen_count.keys(), index=food_allergens.index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens['allergens'] = food_allergens.allergens.apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making one hot encoding labels for each of the allergen we need to consider in the dataset\n",
    "for index in food_allergens.index:\n",
    "    temp_allergens_df.loc[index, food_allergens.loc[index].allergens] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens = pd.concat([food_allergens, temp_allergens_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens.head() # we can see that dummy features have been added at the end of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting and flattening the ingredients text to see how many unique ingredients are there in  the total dataset\n",
    "food_allergens['ingredients_text'] = food_allergens['ingredients_text'].apply((lambda x: split_string_to_list(x)))\n",
    "food_allergens['ingredients_text'] = food_allergens['ingredients_text'].apply(frozenset)\n",
    "\n",
    "# Removing all the duplicates from each of the product\n",
    "unique_ingredients = frozenset.union(*food_allergens['ingredients_text'])\n",
    "\n",
    "print(\"The total number of unique ingredients in the dataset are: {}\".format(len(unique_ingredients)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, there are 186516 features which are 60 times more than the number of unique allergens (approx 3000). We will need to group these ingredients together under the same umbrella. For this, we are going to go through each of the unique ingredients datasets and get only nouns out from them, and then use word2vec and nltk libraries to group similar ingredients under the higher level ingredient. \n",
    "\n",
    "We haven't pursued this task since for this we would need to train a model on all ingredients and this comes under feature engineering and modelling aspect which we will pursue for Milestone 3.\n",
    "Google translate was used to translate the text feature files, hence the translations are not accurate since they contained several languages, which makes the automatic translation task be difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the features which are not needed\n",
    "food_facts_data.drop(['url','image_url', 'image_small_url',\n",
    "       'image_ingredients_url', 'image_ingredients_small_url',\n",
    "       'image_nutrition_url', 'image_nutrition_small_url',\n",
    "        'created_t','last_modified_t', 'generic_name','packaging_tags',\n",
    "        'manufacturing_places_tags','emb_codes', 'emb_codes_tags','first_packaging_code_geo',\n",
    "        'cities_tags','countries_tags','no_nutriments', 'additives_tags','cities','ingredients_from_palm_oil',\n",
    "        'ingredients_that_may_be_from_palm_oil','nutrition_grade_uk', 'nova_group','energy_100g'],\n",
    "         axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as csv file\n",
    "food_facts_data.to_csv('food_facts.csv')\n",
    "food_allergens.to_csv('food_allergens.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We end Milestone 2 here. From here, onwards we follow up with Milestone and the research questions mentioned in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Allergens\n",
    "\n",
    "In this part of the notebook, we are going to group the allergens into a small categories so that we can be more specific as to how each product has what kind of allergen. The allergens were translated before using Google translate and some manual data cleaning, although it was kind of tedious job since allergens or the text in general is in many languages in the dataset.\n",
    "\n",
    "For example: milk, milk solids would come under the `milk` related allergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Putting in the imports for Milestone 3\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the file with all the translated allergens\n",
    "translated_words = []\n",
    "\n",
    "with open('./english_allergens.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        translated_words.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of english translated allergens: {}'.format(len(translated_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are converting each of the translated allergens into lowercase for consistency, stripping any allergens which are completely made up of numbers, stripping them of spaces and some of the allergens have question marks and other characters and cleaning them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a bit of data cleaning on the allergens to make them more consistent\n",
    "translated_words = [word.lower() for word in translated_words]\n",
    "translated_words = [re.sub(r'\\b[0-9]+\\b\\s*', '', word).strip().rstrip() for word in translated_words]\n",
    "translated_words = [word.replace('?', '').replace('-', ' ').replace(\"'\", '') for word in translated_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many allergens are single words\n",
    "total_single_words = 0\n",
    "for word in translated_words:\n",
    "    if len(word.split(' ')) != 1:\n",
    "        continue\n",
    "    else:\n",
    "        total_single_words += 1\n",
    "        \n",
    "print('Total number of allergens which are single word: {}'.format(total_single_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from above, we know that out of 3168 allergens; about 1517 allergens are single words. So they can be easily fed into the `Word2Vec` model, but we need to take care of the allergens which are **made up of 2 or more words**. \n",
    "\n",
    "Below, we try to remove the duplicates since many of the translated allergens end up being translated to the same context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_words = {}\n",
    "for word in translated_words:\n",
    "    single_words[word] = word.split(' ')\n",
    "\n",
    "cnt_single_words = Counter(list(itertools.chain(*single_words.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the NLTK library for Part of Speech tagging since we are only interested in Nouns and not verbs, adjectives etc. So, we tag each word according to what category it belongs to and remove the ones which are not nouns. Also, consider only words which are made up of characters not empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the NLTK library for Part of Speech tagging\n",
    "for word in single_words:\n",
    "    if not word:\n",
    "        continue\n",
    "    single_words[word] = [w for w in single_words[word] if w]\n",
    "    single_words[word] = nltk.pos_tag(single_words[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in single_words:\n",
    "    result = []\n",
    "    if not any(single_words[word]):\n",
    "        single_words[word] = result\n",
    "    else:\n",
    "        for w, tag in single_words[word]:\n",
    "            if tag.startswith('NN') or tag.startswith('JJ'):\n",
    "                result.append((w, tag))\n",
    "            single_words[word] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that some of the allergens are actually additives and these additives generally have a format in which they are written in. For example, `e223`, we remove all of these entries from the dictionary and put them in a list which is labelled for additives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_additives = []\n",
    "\n",
    "for word in single_words.copy():\n",
    "    if not bool(re.match(r\"e[0-9]{3}\", word)):\n",
    "        continue\n",
    "    else:\n",
    "        all_additives.append(word)\n",
    "        del single_words[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove entries from the dictionary which are of length 1 or 2, since they do not make any sense and hence would be easier for the Word2Vec model to train without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove allergens which are of single length or length 2\n",
    "for word in single_words.copy():\n",
    "    if len(word) > 2 and single_words[word]:\n",
    "        continue\n",
    "    else:\n",
    "        del single_words[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the pre existing word2Vec model which has been trained by GLoVE and load it. This has been primarily been trained on Wikipedia dataset and news articles. We will use this model to generate the vector for each of our allergens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_output_file = 'data/word2vec_pretrained.txt'\n",
    "\n",
    "# Only change the glove file to word2vec file if it does not exist\n",
    "if not os.path.exists('data/word2vec_pretrained.txt'):\n",
    "    glove2word2vec('data/glove.840B.300d.txt', word2vec_output_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "print('Number of vocab unique words: {}'.format(len(model.wv.vocab.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the words which have vectors, we load the vector from the model otherwise we append them to the others list since these are words not present in the model and the `Others` tag can be associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the word embeddings for each of the unique words present\n",
    "other_words = []\n",
    "\n",
    "total_words = []\n",
    "for word in single_words:\n",
    "    total_words.extend(word.split(' '))\n",
    "total_words = set(total_words)\n",
    "\n",
    "word_embedding_matrix = dict()\n",
    "for word in total_words:\n",
    "    try:\n",
    "        word_embedding_matrix[word] = model.wv.__getitem__(word)\n",
    "    except KeyError:\n",
    "        other_words.append(word)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the Word2Vec vectors are in 300 dimensions. To visualize them, we need to reduce them to 2 dimensions and see what clusters are made up of them. For this, we use the algorithm called `TSNE`. This basically clusters based on the distance and taking in account the local and global context of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in word_embedding_matrix:\n",
    "        tokens.append(word_embedding_matrix[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    # Perplexity takes into account the global and local features\n",
    "    # We are using dimensionality reduciton for 2 features and taking 2500 iterations into account\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, n_iter=2500, random_state=0)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(20, 20)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, our Word2Vec works, a really interesting result!.  This can be seen that the chemicals are grouped together on the bottom right, the seafood are grouped together in the upper center of the graph, and we can also see examples that `dehydrated` and `dried` go together, `scotch`, `beer`, `ales` are also grouped together. Hence, our Word2Vec is able to group words of the same context together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_single_words = single_words.copy()\n",
    "others_present_in_single = []\n",
    "\n",
    "for word in temp_single_words:\n",
    "    for w, t in temp_single_words[word]:\n",
    "        if not w in other_words:\n",
    "            continue\n",
    "        else:\n",
    "            if len(single_words[word]) == 1:\n",
    "                del single_words[word]\n",
    "            else:\n",
    "                others_present_in_single.append(w)\n",
    "                single_words[word] = [(i, tag) for i, tag in single_words[word] if w != i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_words = set(other_words) - set(others_present_in_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in single_words:\n",
    "    single_words[word] = [(w, t, word_embedding_matrix[w]) for w, t in single_words[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allergens_embedding_matrix = dict()\n",
    "\n",
    "for word in single_words:\n",
    "    allergens_embedding_matrix[word] = sum([e for w, t, e in single_words[word]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, although we have Word2Vec vectors and they have been clustered correctly on the basis of context. We would like to cluster these words in the allergens together on the basis of `similarity` (cosine similarity metric). For this, we use the K Means clustering algorithm with with cosine as the distance metric. Once, we have assigned them to clusters we assign each of the labels to each word so that we can relate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_clusters = 5\n",
    "# Applying Kmeans clusters on vectors  and making 10 clusters out of it\n",
    "k_means = KMeansClusterer(total_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = k_means.cluster(allergens_embedding_matrix.values(), assign_clusters=True)\n",
    "print(assigned_clusters[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_cluster_dict = dict()\n",
    "for i, word in enumerate(allergens_embedding_matrix.keys()):  \n",
    "    assigned_cluster_dict[word] =  assigned_clusters[i]\n",
    "    \n",
    "# Add additives and others to assigned cluster dict\n",
    "assigned_cluster_dict.update(dict.fromkeys(all_additives, 5))\n",
    "assigned_cluster_dict.update(dict.fromkeys(other_words, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the labels for each cluster and assign labels to it\n",
    "assigned_labels = pd.DataFrame.from_dict(assigned_cluster_dict, orient='index')\n",
    "assigned_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we have done clustering on each of the allergens and classified them into 7 categories.\n",
    "\n",
    "0. This contains all kind of `wheat`, `flour`\n",
    "1. Mostly milk, fruit and nut related products like `butter`, `orange`, `egg`\n",
    "2. Mostly powdered related products `whey powder` or related to proteins - like `milk protein`, `whey protein`\n",
    "3. All kinds of acids - like `bisulfite`, `lactate`, `lupine`\n",
    "4. Mostly sea food related allergens - such as `shellfish`, `capelin`\n",
    "5. Additives\n",
    "6. Others - which we have been unable to translate or which not present in the Word2Vec vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0: 'wheat/flour', 1: 'milk/fruit/nut', 2: 'powder/protein',\n",
    "    3: 'acids', 4: 'seafood', 5: 'additives', 6: 'others'\n",
    "}\n",
    "\n",
    "# Assigning the grouped labels we found each\n",
    "assigned_labels['grouped'] = assigned_labels[0].apply(lambda x: labels[x])\n",
    "assigned_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_labels(x):\n",
    "    result = []\n",
    "    for i in x:\n",
    "        print(i, x)\n",
    "        result.append(assigned_labels[assigned_labels.index == i].values[0][1])\n",
    "    return result\n",
    "\n",
    "food_allergens['grouped_allergens'] = food_allergens['allergens'].apply(lambda x: get_grouped_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
