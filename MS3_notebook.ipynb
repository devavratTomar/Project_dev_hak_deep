{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global imports\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sn\n",
    "# Please install langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_data = pd.pandas.read_csv('data/en.openfoodfacts.org.products.csv', sep='\\t', parse_dates=True, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a lot of NaN values in the dataset\n",
    "food_facts_data.isnull().mean().sort_values(ascending=True).plot.barh(figsize = (8, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First idea\n",
    "We are very curious in \"Food carbon footprint\": greenhouse gas emissions produced by growing, rearing, farming, processing, transporting, storing, cooking and disposing of the food we eat. We are interested in finding out the countries, which are emitting huge amount of carbon in the result of food production and consumption, and identify the ways to reduce it. We hope the dataset will give us insights about the products and their origins having highest carbon footprint(i.e. meat, cheese,eggs, so on) and the ones having lower carbon footprints(i.e. fruit, vegetables, beans, nuts so on). Additionally we consider that food packaging and food waste treatment have huge proportion of impact on carbon emission, thus these were hypothesis that we were interested to test and make conclusions about global problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARBON_FOOD_PRINT_MASK = ~food_facts_data['carbon-footprint_100g'].isnull()\n",
    "print(\"Number of food samples with valid carbon foot print is {}\".format(food_facts_data[CARBON_FOOD_PRINT_MASK].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Carbon-foot-print.\n",
    "food_facts_data['carbon-footprint_100g'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = food_facts_data.dtypes[food_facts_data.dtypes ==  'float64'].index\n",
    "\n",
    "# Plotting the correlation matrix for carbon food print\n",
    "corr_ = food_facts_data[CARBON_FOOD_PRINT_MASK][numeric_columns].corr().abs()\n",
    "fig, axis = plt.subplots(figsize=(10, 8))\n",
    "sn.heatmap(corr_, mask=np.zeros_like(corr_, dtype=np.bool), cmap=sn.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort the features having highest correlation with carbon footprint\n",
    "valid_corr_columns = corr_['carbon-footprint_100g'].dropna().sort_values(ascending=False)\n",
    "valid_corr_columns = valid_corr_columns[valid_corr_columns > 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out number of data samples with high-correlation with other non-numeric columns.\n",
    "food_facts_data[food_facts_data['carbon-footprint_100g'].notnull()][valid_corr_columns.index].notnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Unfortunately the high correlation is because of the lack of datapoints, it has been shown that the proportions of NAN is very high in the dataset where the carbon footprint has some information. So our thoughts that we may apply regression and other fill in tools to fill the carbon footprint values are ruined, we decided to leave this idea because we dont have enough data and didn't find the carbon footprint of each food product to make real conculsions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch to the second idea\n",
    " Our final idea is  \"Food allergy\", which happens when human body overreact to exposure to particular substances in the food.\n",
    "Our aim in this idea is to identify the possible allergies that unlabeled food may cause and recommend the foods that has specific nutrition  and aren't affecting to people with specific allergies.\n",
    "The social goodness will be to help food allergic people find their healthy and enough proportion of  food easily.\n",
    "\n",
    "\n",
    "So we will mostly observe the Additives, Ingredients, Nutrients, Allergens and preprocess them for our next milestone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation of dataset each food item is assigned a unique code. So we can make code as the index of our data frame and relate all the other datasets on this index.\n",
    "\n",
    "However, we find that the code is not unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    food_facts_data.set_index('code', inplace=True, drop=False)\n",
    "except:\n",
    "    print('code is already index')\n",
    "    \n",
    "print(\"Is the dataset index unique? {}\".format(food_facts_data.index.is_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets investigate the data samples with the same bar code.\n",
    "_not_unique = food_facts_data[['code']].groupby(food_facts_data.index).count() != 1\n",
    "food_facts_data.loc[_not_unique[_not_unique.code].index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data contains duplicate values for all columns, except for last_modified time. Since the food product is same we pick the latest samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_data.drop(columns=['code'], inplace=True)\n",
    "food_facts_data = food_facts_data.groupby(food_facts_data.index).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is index unique: {}\".format(food_facts_data.index.is_unique))\n",
    "print(\"Number of na index: {}\".format(food_facts_data.index.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the na code food item as the data is corrupted\n",
    "food_facts_data = food_facts_data.drop(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of na index: {}\".format(food_facts_data.index.isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first analyse the nutritional facts of all the available products. So we keep columns 'product_name', 'categories'\n",
    "and all nutrition features (number of features: 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTRIENTS_ALL = [\n",
    "       'energy_100g', 'energy-from-fat_100g', 'fat_100g',\n",
    "       'saturated-fat_100g', '-butyric-acid_100g', '-caproic-acid_100g',\n",
    "       '-caprylic-acid_100g', '-capric-acid_100g', '-lauric-acid_100g',\n",
    "       '-myristic-acid_100g', '-palmitic-acid_100g', '-stearic-acid_100g',\n",
    "       '-arachidic-acid_100g', '-behenic-acid_100g',\n",
    "       '-lignoceric-acid_100g', '-cerotic-acid_100g',\n",
    "       '-montanic-acid_100g', '-melissic-acid_100g',\n",
    "       'monounsaturated-fat_100g', 'polyunsaturated-fat_100g',\n",
    "       'omega-3-fat_100g', '-alpha-linolenic-acid_100g',\n",
    "       '-eicosapentaenoic-acid_100g', '-docosahexaenoic-acid_100g',\n",
    "       'omega-6-fat_100g', '-linoleic-acid_100g',\n",
    "       '-arachidonic-acid_100g', '-gamma-linolenic-acid_100g',\n",
    "       '-dihomo-gamma-linolenic-acid_100g', 'omega-9-fat_100g',\n",
    "       '-oleic-acid_100g', '-elaidic-acid_100g', '-gondoic-acid_100g',\n",
    "       '-mead-acid_100g', '-erucic-acid_100g', '-nervonic-acid_100g',\n",
    "       'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g',\n",
    "       'sugars_100g', '-sucrose_100g', '-glucose_100g', '-fructose_100g',\n",
    "       '-lactose_100g', '-maltose_100g', '-maltodextrins_100g',\n",
    "       'starch_100g', 'polyols_100g', 'fiber_100g', 'proteins_100g',\n",
    "       'casein_100g', 'serum-proteins_100g', 'nucleotides_100g',\n",
    "       'salt_100g', 'sodium_100g', 'alcohol_100g', 'vitamin-a_100g',\n",
    "       'beta-carotene_100g', 'vitamin-d_100g', 'vitamin-e_100g',\n",
    "       'vitamin-k_100g', 'vitamin-c_100g', 'vitamin-b1_100g',\n",
    "       'vitamin-b2_100g', 'vitamin-pp_100g', 'vitamin-b6_100g',\n",
    "       'vitamin-b9_100g', 'folates_100g', 'vitamin-b12_100g',\n",
    "       'biotin_100g', 'pantothenic-acid_100g', 'silica_100g',\n",
    "       'bicarbonate_100g', 'potassium_100g', 'chloride_100g',\n",
    "       'calcium_100g', 'phosphorus_100g', 'iron_100g', 'magnesium_100g',\n",
    "       'zinc_100g', 'copper_100g', 'manganese_100g', 'fluoride_100g',\n",
    "       'selenium_100g', 'chromium_100g', 'molybdenum_100g', 'iodine_100g',\n",
    "       'caffeine_100g', 'taurine_100g', 'ph_100g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts = food_facts_data[NUTRIENTS_ALL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that some nutrients have only very few data points. (For example -butyric-acid has only one data point)\n",
    "So, we keep only top KEEP_NUTRITION nutrients at the moment so that count is greater than 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nutrition = nutrition_facts.count().sort_values(ascending=False)\n",
    "count_nutrition = count_nutrition[count_nutrition>1000.]\n",
    "count_nutrition.plot.barh(figsize=(16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTRIENTS_FEATURES = count_nutrition.index.values.tolist()\n",
    "# Now we have 36 nutrients columns.\n",
    "# 'energy_100g', 'proteins_100g', 'fat_100g', 'carbohydrates_100g', 'sugars_100g', 'salt_100g', 'sodium_100g',\n",
    "# 'saturated-fat_100g', 'fiber_100g', 'cholesterol_100g', 'trans-fat_100g', 'calcium_100g', 'vitamin-c_100g', 'iron_100g',\n",
    "# 'vitamin-a_100g', 'potassium_100g', 'polyunsaturated-fat_100g', 'monounsaturated-fat_100g', 'vitamin-pp_100g', \n",
    "# 'vitamin-b1_100g', 'vitamin-b2_100g', 'alcohol_100g', 'vitamin-d_100g', 'vitamin-b6_100g', 'magnesium_100g', 'phosphorus_100g',\n",
    "# 'vitamin-b12_100g', 'vitamin-b9_100g', 'zinc_100g', 'folates_100g', 'pantothenic-acid_100g', 'copper_100g', 'vitamin-e_100g',\n",
    "# 'manganese_100g', 'selenium_100g', 'omega-3-fat_100g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts = nutrition_facts[NUTRIENTS_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also observe many outliers in the dataset. For example, for some products proteins_100g value is 31000 which should not be\n",
    "possible as we are measuring the quantities in per 100 g. One possible explaination would be that the units of measurements\n",
    "are not consistent with other samples. Also, some nutrients values are negative. These values may be as a result of error in\n",
    "interpreting - sign as negative.\n",
    "Presently, we remove these outliers from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    # First remove the negative sign\n",
    "    df = df.abs()\n",
    "    \n",
    "    # As the weight of the food item is 100g, the weight of nutrients can never exceed 100g.\n",
    "    threshold_cutoff = pd.Series(100*np.ones(df.columns.size), index=df.columns)\n",
    "    \n",
    "    # Since energy_100g measured in kilo joules(kJ) we don't clip it. However the highest energy is given by fats (37kJ/g).\n",
    "    # So the upper bound of energy per 100g should be 3700kJ.\n",
    "    threshold_cutoff.loc['energy_100g'] = 40*1e2\n",
    "    \n",
    "    return df[df <= threshold_cutoff].dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_filt = remove_outliers(nutrition_facts)\n",
    "print(\"Nutrition data shape {}\".format(nutrition_facts_filt.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_filt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions_nutrition(df, nlog_scale_col):\n",
    "    fig, axes = plt.subplots(9, len(df.columns)//9, figsize=(30, 30))\n",
    "    axes = axes.flatten()\n",
    "    for col, axis in zip(df.columns, axes):\n",
    "        bins = np.linspace(df[col].min(), df[col].quantile(0.99), 50)\n",
    "        df.hist(column = col, bins = bins, ax=axis)\n",
    "        if not col in nlog_scale_col:\n",
    "            axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions_nutrition(nutrition_facts_filt, nlog_scale_col=['energy_100g', 'proteins_100g', 'fat_100g',\n",
    "                                                                  'carbohydrates_100g', 'sugars_100g', \n",
    "                                                                   'omega-3-fat_100g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a nutrition value is not mentioned on the label (NaN), we can assume that the nutrient is absent in the food item.\n",
    "Thus we replace all NaN with 0. This is because in the absence of any nutrition value, we have no way to know if the product\n",
    "actually has that nutrient. For the outlier products, our proposed solution of recommending alternate food product\n",
    "based on nutrients (for allergies and additives) can give wrong results. But we see that this problem even arises with OpenFood\n",
    "website that calcultes nutri-score wrong if outliers are present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_values(df):\n",
    "    df[df.columns.drop('energy_100g')] = df[df.columns.drop('energy_100g')].replace(np.NaN, 0)\n",
    "    return df\n",
    "\n",
    "nutrition_facts_filt = remove_nan_values(nutrition_facts_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of nutrition data after missing values removal {}\".format(nutrition_facts_filt[nutrition_facts_filt.energy_100g.isna()].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Observe that there are 6424 products with unknown energy_100g. \n",
    "So, for these products we can calculate approximate energy.\n",
    "\n",
    "\n",
    "$${energy{100g}} = 17*{protein{100g}} + 37*{fat{100g}} + 17*{carbohydrate{100g}} + 8*{fibre{100g}} + 29*{alcohol_{100g}}$$\n",
    "\n",
    "\n",
    "source: http://www.mydailyintake.net/energy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_energy_nan(df):\n",
    "    na_mask = df['energy_100g'].isna()\n",
    "    df.loc[df.index[na_mask], 'energy_100g'] = \\\n",
    "                        17*(df[na_mask].proteins_100g + df[na_mask].carbohydrates_100g) +\\\n",
    "                        37*df[na_mask].fat_100g + 8*df[na_mask].fiber_100g + 29*df[na_mask].alcohol_100g\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_filt = remove_energy_nan(nutrition_facts_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_data.drop(NUTRIENTS_ALL, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all the changes to the original data\n",
    "food_facts_data = food_facts_data.merge(nutrition_facts_filt, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts = pd.read_csv('data/food_facts.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do some research on how ingredients text and allergens are like and what transformation do we need to perform for each one of them.\n",
    "\n",
    "For MS2, we considered using both `ingredients` and `allergens` but since the ingredients is not consistent we will take `categories_en` instead. So, the below cell does not filter ingredients_text anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the data where ingredients text and allergens are applicable\n",
    "# food_allergens_mask = food_facts[\"ingredients_text\"].notnull() & food_facts[\"allergens\"].notnull()\n",
    "food_allergens_mask = food_facts[\"allergens\"].notnull()\n",
    "food_allergens = food_facts[food_allergens_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of products is: {}\".format(food_facts.shape[0]))\n",
    "print(\"The total number of products with food allergens ando ingredients data is: {}\".format(food_allergens.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****This is approximately 10% of the dataset and hence we have more training data then carbon footprint****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the language for a particular food / allergen product - as to which language is it written in\n",
    "def detect_language(phrase):\n",
    "    try:        \n",
    "        if not phrase:\n",
    "            return 'No language'\n",
    "        \n",
    "        language_id = detect(phrase)\n",
    "        return language_id\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens['language_id'] = food_allergens[\"ingredients_text\"][:2000].apply(lambda x: detect_language(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we take the first 2000 samples to see how many languages are there\n",
    "As, we can see there are 22 languages for ingredients_text which is a ***very difficult translation task in its own!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens.groupby('language_id').count()['key_0'].plot.barh()\n",
    "plt.title(\"The frequency of used Languages\")\n",
    "food_allergens = food_allergens.drop(columns=[\"language_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now trying to find the number of languages present in the allergens column\n",
    "food_allergens['language_id'] = food_allergens[\"allergens\"][:2000].apply(lambda x: detect_language(x))\n",
    "food_allergens.groupby('language_id').count()['key_0'].plot.barh()\n",
    "plt.title(\"the number of languages present in the allergens column\")\n",
    "food_allergens = food_allergens.drop(columns=[\"language_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allergens_dict = {}\n",
    "with open(\"original_allergens.txt\") as f, open(\"english_allergens.txt\") as f2:\n",
    "    for x, y in zip(f, f2):\n",
    "        allergens_dict[x.lower().replace('\\n', '').strip().rstrip()] = re.sub(' +', ' ', y).lower().replace('\\n', '').strip().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_to_list(phrase, match_dict=None):\n",
    "    # Sometimes the word has a incorrect language tag in front of it\n",
    "    # So we are removing all tags and using language detection instead\n",
    "    foods = []\n",
    "\n",
    "    for food in phrase.split(','):\n",
    "        food = food.strip().rstrip()\n",
    "        \n",
    "        # Delete all additives since additives are nearly defined for the whole dataset\n",
    "        food = re.sub(' +', ' ', food).lower()\n",
    "        food = re.sub(r\"[^A-Za-zàâçéèêëîïôûùüÿñæœ\\'\\s,]\", '', food)\n",
    "        \n",
    "        # or food items which are made up of one characters\n",
    "        if(len(food) <= 2):\n",
    "            continue\n",
    "        \n",
    "        # Split the tag - en, fr if they have it\n",
    "        formatted_food = food.split(':')[1] if ':' in food else food\n",
    "    \n",
    "        if match_dict and formatted_food in match_dict:\n",
    "            translated_food = match_dict[formatted_food]\n",
    "        else:\n",
    "            translated_food = formatted_food\n",
    "        foods.append(translated_food)\n",
    "    return foods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transferring all french allergens or allergens in any other language to the english language from the translated files we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens['allergens'] = food_allergens['allergens'].apply((lambda x: split_string_to_list(x, match_dict=allergens_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens['allergens'] = food_allergens['allergens'].apply(frozenset) # Removing all the duplicates from each of the product\n",
    "unique_allergens = frozenset.union(*food_allergens['allergens'])\n",
    "\n",
    "print(\"The total number of food allergens before cleaning them for less than 5 products: {}\".format(len(unique_allergens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we are using the `food_allergens` variable below in MS3, we make a duplicate variable so that MS2 analysis and processing can still work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_food_allergens = food_allergens.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we get the allergy count for each of the allergies which are present in more than 5 products\n",
    "allergen_count = {}\n",
    "temp_allergens = list(unique_allergens)\n",
    "\n",
    "# Calculating the allergens and removing the allergens which are present in less than 5 products\n",
    "for allergen in unique_allergens:\n",
    "    count = temp_food_allergens['allergens'].apply(lambda x: int(allergen in x)).sum()\n",
    "    \n",
    "    # Remove allergen if it is present in less than 5 products\n",
    "    if count < 5:\n",
    "        temp_allergens.remove(allergen)\n",
    "        continue\n",
    "    allergen_count[allergen] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(allergen_count).sort_values()[-20:].plot.barh() \n",
    "plt.title(\"top most 20 allergens in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of allergens after cleaning them by considering them for greater than 5 products: {}'.format(\n",
    "    len(allergen_count.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the intersection of all allergens which coexist with \n",
    "allergen_set = frozenset(allergen_count.keys())\n",
    "temp_food_allergens['allergens'] = temp_food_allergens['allergens'].apply(lambda x: allergen_set.intersection(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_allergens_df = pd.DataFrame(pd.np.empty((temp_food_allergens.shape[0],len(allergen_set))) * 0, columns=allergen_count.keys(), index=food_allergens.index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_food_allergens['allergens'] = temp_food_allergens.allergens.apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making one hot encoding labels for each of the allergen we need to consider in the dataset\n",
    "for index in temp_food_allergens.index:\n",
    "    temp_allergens_df.loc[index, temp_food_allergens.loc[index].allergens] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_food_allergens = pd.concat([temp_food_allergens, temp_allergens_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_food_allergens.head() # we can see that dummy features have been added at the end of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting and flattening the ingredients text to see how many unique ingredients are there in  the total dataset\n",
    "temp_food_allergens['ingredients_text'] = temp_food_allergens['ingredients_text'].apply((lambda x: split_string_to_list(x)))\n",
    "temp_food_allergens['ingredients_text'] = temp_food_allergens['ingredients_text'].apply(frozenset)\n",
    "\n",
    "# Removing all the duplicates from each of the product\n",
    "unique_ingredients = frozenset.union(*temp_food_allergens['ingredients_text'])\n",
    "\n",
    "print(\"The total number of unique ingredients in the dataset are: {}\".format(len(unique_ingredients)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, there are 186516 features which are 60 times more than the number of unique allergens (approx 3000). We will need to group these ingredients together under the same umbrella. For this, we are going to go through each of the unique ingredients datasets and get only nouns out from them, and then use word2vec and nltk libraries to group similar ingredients under the higher level ingredient. \n",
    "\n",
    "We haven't pursued this task since for this we would need to train a model on all ingredients and this comes under feature engineering and modelling aspect which we will pursue for Milestone 3.\n",
    "Google translate was used to translate the text feature files, hence the translations are not accurate since they contained several languages, which makes the automatic translation task be difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the features which are not needed\n",
    "food_facts_data.drop(['url','image_url', 'image_small_url',\n",
    "       'image_ingredients_url', 'image_ingredients_small_url',\n",
    "       'image_nutrition_url', 'image_nutrition_small_url',\n",
    "        'created_t','last_modified_t', 'generic_name','packaging_tags',\n",
    "        'manufacturing_places_tags','emb_codes', 'emb_codes_tags','first_packaging_code_geo',\n",
    "        'cities_tags','countries_tags','no_nutriments', 'additives_tags','cities','ingredients_from_palm_oil',\n",
    "        'ingredients_that_may_be_from_palm_oil','nova_group','energy_100g'],\n",
    "         axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as csv file\n",
    "# food_facts_data.to_csv('food_facts.csv')\n",
    "# food_allergens.to_csv('food_allergens.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We end Milestone 2 here. From here, onwards we follow up with Milestone and the research questions mentioned in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Allergens\n",
    "\n",
    "In this part of the notebook, we are going to group the allergens into a small categories so that we can be more specific as to how each product has what kind of allergen. The allergens were translated before using Google translate and some manual data cleaning, although it was kind of tedious job since allergens or the text in general is in many languages in the dataset.\n",
    "\n",
    "For example: milk, milk solids would come under the `milk` related allergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Putting in the imports for Milestone 3\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we load the translations for each of the allergens using the `english_allergens` file below. As said before, the translations have been done by first detecting the language and then using Google translate and doing some data cleaning by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the file with all the translated allergens\n",
    "translated_words = []\n",
    "\n",
    "with open('./english_allergens.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        translated_words.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of english translated allergens: {}'.format(len(translated_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are converting each of the translated allergens into lowercase for consistency, stripping any allergens which are completely made up of numbers, stripping them of spaces and some of the allergens have question marks and other characters and cleaning them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a bit of data cleaning on the allergens to make them more consistent\n",
    "translated_words = [word.lower() for word in translated_words]\n",
    "translated_words = [re.sub(r'\\b[0-9]+\\b\\s*', '', word).strip().rstrip() for word in translated_words]\n",
    "translated_words = [word.replace('?', '').replace('-', ' ').replace(\"'\", '') for word in translated_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many allergens are single words\n",
    "total_single_words = 0\n",
    "for word in translated_words:\n",
    "    if len(word.split(' ')) != 1:\n",
    "        continue\n",
    "    else:\n",
    "        total_single_words += 1\n",
    "        \n",
    "print('Total number of allergens which are single word: {}'.format(total_single_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from above, we know that out of 3168 allergens; about 1517 allergens are single words. So they can be easily fed into the `Word2Vec` model, but we need to take care of the allergens which are **made up of 2 or more words**. \n",
    "\n",
    "Below, we try to remove the duplicates since many of the translated allergens end up being translated to the same context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_words = {}\n",
    "for word in translated_words:\n",
    "    single_words[word] = word.split(' ')\n",
    "\n",
    "cnt_single_words = Counter(list(itertools.chain(*single_words.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the NLTK library for Part of Speech tagging since we are only interested in Nouns and not verbs, adjectives etc. So, we tag each word according to what category it belongs to and remove the ones which are not nouns. Also, consider only words which are made up of characters not empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the NLTK library for Part of Speech tagging\n",
    "for word in single_words:\n",
    "    if not word:\n",
    "        continue\n",
    "    single_words[word] = [w for w in single_words[word] if w]\n",
    "    single_words[word] = nltk.pos_tag(single_words[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in single_words:\n",
    "    result = []\n",
    "    if not any(single_words[word]):\n",
    "        single_words[word] = result\n",
    "    else:\n",
    "        # Just taking into account the nouns and adjectives\n",
    "        for w, tag in single_words[word]:\n",
    "            if tag.startswith('NN') or tag.startswith('JJ'):\n",
    "                result.append((w, tag))\n",
    "            single_words[word] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that some of the allergens are actually additives and these additives generally have a format in which they are written in. For example, `e223`, we remove all of these entries from the dictionary and put them in a list which is labelled for additives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_additives = []\n",
    "\n",
    "for word in single_words.copy():\n",
    "    # Just taking additives into account which are written in the right format\n",
    "    if not bool(re.match(r\"e[0-9]{3}\", word)):\n",
    "        continue\n",
    "    else:\n",
    "        all_additives.append(word)\n",
    "        del single_words[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove entries from the dictionary which are of length 1 or 2, since they do not make any sense and hence would be easier for the Word2Vec model to train without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove allergens which are of single length or length 2\n",
    "for word in single_words.copy():\n",
    "    if len(word) > 2 and single_words[word]:\n",
    "        continue\n",
    "    else:\n",
    "        del single_words[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the pre existing word2Vec model which has been trained by GLoVE and load it. This has been primarily been trained on Wikipedia dataset and news articles. We will use this model to generate the vector for each of our allergens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_output_file = 'data/word2vec_pretrained.txt'\n",
    "\n",
    "# Only change the glove file to word2vec file if it does not exist\n",
    "if not os.path.exists('data/word2vec_pretrained.txt'):\n",
    "    glove2word2vec('data/glove.840B.300d.txt', word2vec_output_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "print('Number of vocab unique words: {}'.format(len(model.wv.vocab.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the words which have vectors, we load the vector from the model otherwise we append them to the others list since these are words not present in the model and the `Others` tag can be associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the all the unique words present in the translated allergens by splitting them by space\n",
    "other_words = []\n",
    "total_words = []\n",
    "for word in single_words:\n",
    "    total_words.extend(word.split(' '))\n",
    "total_words = set(total_words)\n",
    "\n",
    "## Getting the word embeddings for each of the unique words present\n",
    "word_embedding_matrix = dict()\n",
    "for word in total_words:\n",
    "    try:\n",
    "        word_embedding_matrix[word] = model.wv.__getitem__(word)\n",
    "    except KeyError:\n",
    "        other_words.append(word)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the Word2Vec vectors are in 300 dimensions. To visualize them, we need to reduce them to 2 dimensions and see what clusters are made up of them. For this, we use the algorithm called `TSNE`. This basically clusters based on the distance and taking in account the local and global context of the vectors (using the perplexity). The algorithm is basically run for 2500 iterations so that it can fit properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in word_embedding_matrix:\n",
    "        tokens.append(word_embedding_matrix[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    # Perplexity takes into account the global and local features\n",
    "    # We are using dimensionality reduciton for 2 features and taking 2500 iterations into account\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, n_iter=2500, random_state=0)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(20, 20)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                     textcoords='offset points', ha='right', va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, our Word2Vec works, a really interesting result!.  This can be seen that the chemicals are grouped together on the bottom right, the seafood are grouped together in the upper center of the graph, and we can also see examples that `dehydrated` and `dried` go together, `scotch`, `beer`, `ales` are also grouped together. Hence, our Word2Vec is able to group words of the same context together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_single_words = single_words.copy()\n",
    "others_present_in_single = []\n",
    "\n",
    "# Delete words if the length of the word is 1 or if it is not present in other words\n",
    "for word in temp_single_words:\n",
    "    for w, t in temp_single_words[word]:\n",
    "        if not w in other_words:\n",
    "            continue\n",
    "        else:\n",
    "            if len(single_words[word]) == 1:\n",
    "                del single_words[word]\n",
    "            else:\n",
    "                others_present_in_single.append(w)\n",
    "                single_words[word] = [(i, tag) for i, tag in single_words[word] if w != i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_words = set(other_words) - set(others_present_in_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in single_words:\n",
    "    single_words[word] = [(w, t, word_embedding_matrix[w]) for w, t in single_words[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allergens_embedding_matrix = dict()\n",
    "\n",
    "for word in single_words:\n",
    "    allergens_embedding_matrix[word] = sum([e for w, t, e in single_words[word]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, although we have Word2Vec vectors and they have been clustered correctly on the basis of context. We would like to cluster these words in the allergens together on the basis of `similarity` (cosine similarity metric). For this, we use the K Means clustering algorithm with with cosine as the distance metric. Once, we have assigned them to clusters we assign each of the labels to each word so that we can relate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_clusters = 5\n",
    "# Applying Kmeans clusters on vectors  and making 5 clusters out of it\n",
    "k_means = KMeansClusterer(total_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = k_means.cluster(allergens_embedding_matrix.values(), assign_clusters=True)\n",
    "print('The labels for the first twenty items are: {}'.format(assigned_clusters[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the order into account and adding elements into the cluster\n",
    "assigned_cluster_dict = dict()\n",
    "for i, word in enumerate(allergens_embedding_matrix.keys()):  \n",
    "    assigned_cluster_dict[word] =  assigned_clusters[i]\n",
    "    \n",
    "# Add additives and others to assigned cluster dict\n",
    "assigned_cluster_dict.update(dict.fromkeys(all_additives, 5))\n",
    "assigned_cluster_dict.update(dict.fromkeys(other_words, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the labels for each cluster and assign labels to it\n",
    "assigned_labels = pd.DataFrame.from_dict(assigned_cluster_dict, orient='index')\n",
    "assigned_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we have done clustering on each of the allergens and classified them into 7 categories.\n",
    "\n",
    "0. This contains all kind of `wheat`, `flour`\n",
    "1. Mostly milk, fruit and nut related products like `butter`, `orange`, `egg`\n",
    "2. Mostly powdered related products `whey powder` or related to proteins - like `milk protein`, `whey protein`\n",
    "3. All kinds of acids - like `bisulfite`, `lactate`, `lupine`\n",
    "4. Mostly sea food related allergens - such as `shellfish`, `capelin`\n",
    "5. Additives\n",
    "6. Others - which we have been unable to translate or which not present in the Word2Vec vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0: 'wheat/flour', 1: 'milk/fruit/nut', 2: 'powder/protein',\n",
    "    3: 'acids', 4: 'seafood', 5: 'additives_l', 6: 'others'\n",
    "}\n",
    "\n",
    "# Assigning the grouped labels we found each\n",
    "assigned_labels['grouped'] = assigned_labels[0].apply(lambda x: labels[x])\n",
    "assigned_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_labels(x):\n",
    "    result = []\n",
    "    for i in x:\n",
    "        if i not in assigned_labels.index:\n",
    "            continue\n",
    "        else:\n",
    "            result.append(assigned_labels[assigned_labels.index == i].values[0][1])\n",
    "    return result\n",
    "\n",
    "# Assigned grouped allergens - so basically for each allergen assign which allergen it is in\n",
    "food_allergens['grouped_allergens'] = food_allergens['allergens'].apply(lambda x: get_grouped_labels(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after assigning each `allergen` to any of the `7 categories` above, we make a new Dataframe in which we one hot encode these variables and merge them together with the food allergens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making one hot encoding labels for each of the grouped allergen we need to consider in the dataset\n",
    "temp_grouped_allergens = pd.DataFrame(pd.np.empty((food_allergens.shape[0],len(labels.values()))) * 0, columns=labels.values(), index=food_allergens.index) \n",
    "for index in food_allergens.index:\n",
    "    temp_grouped_allergens.loc[index, food_allergens.loc[index].grouped_allergens] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens = pd.concat([food_allergens, temp_grouped_allergens], axis=1)\n",
    "food_allergens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_allergens.set_index('key_0', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Quantisation\n",
    "\n",
    "To recommend products which do not contain a specific allergen but contain the same nutritional value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a class in module `vector_query` that performs `Vector Quantization` on the nutrition feature vectors as described in the report. We create a `Binary Search Tree` for every allergen-type food item by adding new nodes (mean vector of the clusters) at every cluster split step in Vector Quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import vector_query\n",
    "importlib.reload(vector_query)\n",
    "\n",
    "# As discussed in the report we don't use energy_100g feature for creating the binary search tree\n",
    "NUTRIENTS_FEATURES.remove('energy_100g')\n",
    "# The maximum size of number of food items in the leaf node\n",
    "MAX_BIN = 20\n",
    "# If set to False, don't perform VQ and instead load pickle files\n",
    "RUN_BST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_BST:\n",
    "    for aller_type in labels.values():\n",
    "        food_aller_type = food_allergens[food_allergens[aller_type] == 1.0][NUTRIENTS_FEATURES]\n",
    "        vq_food_aller = vector_query.VectorQuantizationBST(food_aller_type, MAX_BIN)\n",
    "        vq_food_aller.run_all()\n",
    "        vector_query.save_nutrition_vq_object(vq_food_aller, 'nutrients_allergens_{}.pickle'.format(aller_type.replace('/','_')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuction `get_food_items` returns the food items that don't have the specified allergen category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_food_items(allergens, nutrition_vector):\n",
    "    ALLERGENS = ['wheat/flour', 'milk/fruit/nut', 'powder/protein', 'acids', 'seafood', 'additives_l', 'others']\n",
    "\n",
    "    for allergen in allergens:\n",
    "        if not allergen in ALLERGENS:\n",
    "            raise ValueError(\"{} is not an allergen category\".format(allergen))\n",
    "    food_items = []\n",
    "    \n",
    "    for aller in ALLERGENS:\n",
    "        if aller in allergens:\n",
    "            continue\n",
    "        vq = vector_query.restore_nutrition_vq_object('nutrients_allergens_{}.pickle'.format(aller.replace('/','_')))\n",
    "        food_items.append(vq.get_similar_food(nutrition_vector))\n",
    "        print('Added items for allergy category {}'.format(aller))\n",
    "    return np.concatenate(food_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of using `get_food_items`. We randomly select a food item from the food_facts data and return all food items that have similar nutritions with the given food allergen (for e.g. `wheat/flour`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for getting food items\n",
    "query = food_allergens[NUTRIENTS_FEATURES].iloc[6001]\n",
    "similar_items = get_food_items(['wheat/flour'], query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_items[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food additives are substances added to food to maintain or improve its safety, freshness, taste, texture, or appearance. Food additives need to be checked for potential harmful effects on human health before they can be used.\n",
    "\n",
    "  \n",
    "  https://www.fda.gov/Food/IngredientsPackagingLabeling/FoodAdditivesIngredients/ucm094211.htm#introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links for additives info:\n",
    "\n",
    "http://www.foodadditivesworld.com/other-food-condiments.html\n",
    "\n",
    "https://www.betterhealth.vic.gov.au/health/ConditionsAndTreatments/food-additives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenColumn(input, column):\n",
    "    '''\n",
    "    column is a string of the column's name.\n",
    "    for each value of the column's element (which might be a list),\n",
    "    duplicate the rest of columns at the corresponding row with the (each) value.\n",
    "    '''\n",
    "    column_flat = pd.DataFrame(\n",
    "        [\n",
    "            [i, c_flattened]\n",
    "            for i, y in input[column].apply(list).iteritems()\n",
    "            for c_flattened in y\n",
    "        ],\n",
    "        columns=['I', column]\n",
    "    )\n",
    "    column_flat = column_flat.set_index('I')\n",
    "    return (\n",
    "        input.drop(column, 1)\n",
    "             .merge(column_flat, left_index=True, right_index=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the not available additives by 'missing'\n",
    "food_facts_data.additives_en.fillna('e000 - missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split multiple additives\n",
    "food_facts_data.additives_en = food_facts_data.additives_en.str.split(',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the additive_en feature to get the products with one additive\n",
    "food_additives_flattened = flattenColumn(food_facts_data, 'additives_en')\n",
    "food_additives_flattened.additives_en = food_additives_flattened.additives_en.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of flattened data: {}'.format(food_additives_flattened.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The number of unique additives in the allergens dataset is {}\".format(food_additives_flattened.additives_en.unique().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting distribution of Additives in the dataset\n",
    "food_additives_flattened.additives_en.value_counts()[:20].plot.barh()\n",
    "plt.title(\"The top 20 additives in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_additives_flattened['additives_code'] = [x[1:4] for x in food_additives_flattened.additives_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_additives_flattened.additives_code.replace(['14x','15x'],['140','150'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_additives_flattened.additives_code = [int(x) for x in food_additives_flattened.additives_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask100_199 = food_additives_flattened.additives_code.isin(range(100, 199))\n",
    "mask200_299 = food_additives_flattened.additives_code.isin(range(200, 299))\n",
    "mask300_399 = food_additives_flattened.additives_code.isin(range(300, 399))\n",
    "mask400_499 = food_additives_flattened.additives_code.isin(range(400, 499))\n",
    "mask500_599 = food_additives_flattened.additives_code.isin(range(500, 599))\n",
    "mask600_699 = food_additives_flattened.additives_code.isin(range(600, 699))\n",
    "mask700_799 = food_additives_flattened.additives_code.isin(range(700, 799))\n",
    "mask900_999 = food_additives_flattened.additives_code.isin(range(900, 999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://en.wikipedia.org/wiki/E_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_additives_flattened['add_purpose'] = 'missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_additives_flattened['add_purpose'][mask100_199] = 'color'\n",
    "food_additives_flattened['add_purpose'][mask200_299] = 'preservatives'\n",
    "food_additives_flattened['add_purpose'][mask300_399] = 'acidity_regulators'\n",
    "food_additives_flattened['add_purpose'][mask400_499] = 'thickeners_stabilisers_emulsifiers'\n",
    "food_additives_flattened['add_purpose'][mask500_599] = 'anticaking_agents'\n",
    "food_additives_flattened['add_purpose'][mask600_699] = 'flavour_enhancers'\n",
    "food_additives_flattened['add_purpose'][mask700_799] = 'antibiotics'\n",
    "food_additives_flattened['add_purpose'][mask900_999] = 'glazingagents_gases_sweeteners'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dummies of the additives \n",
    "dummies_additives = pd.get_dummies(food_additives_flattened.add_purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the products to be unique by groupbying and taking the sum\n",
    "unique_index = dummies_additives.groupby(dummies_additives.index).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the less common additives\n",
    "common_additives = unique_index[unique_index.columns[unique_index.sum()>20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_additives.sum().plot.barh()\n",
    "plt.title(\"The usage of types of additive in products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the flattened additives data with the food data \n",
    "food_facts_data = food_facts_data.merge(\n",
    "    common_additives, left_index=True, right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing product dependencies of additives\n",
    "plt.figure(figsize=(5,35))\n",
    "plt.subplot(711)\n",
    "food_facts_data[food_facts_data.preservatives != 0].product_name.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top products with preservatives\")\n",
    "plt.subplot(712)\n",
    "food_facts_data[food_facts_data.acidity_regulators != 0].product_name.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top products with acidity_regulator additives\")\n",
    "plt.subplot(713)\n",
    "food_facts_data[food_facts_data.anticaking_agents != 0].product_name.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top products with anticaking_agents additives\")\n",
    "plt.subplot(714)\n",
    "food_facts_data[food_facts_data.color != 0].product_name.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top products with color additives\")\n",
    "plt.subplot(715)\n",
    "food_facts_data[food_facts_data.flavour_enhancers != 0].product_name.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top products with flavour_enhancers additives\")\n",
    "plt.subplot(716)\n",
    "food_facts_data[food_facts_data.glazingagents_gases_sweeteners != 0].product_name.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top products with glazingagents_gases_sweeteners additives\")\n",
    "plt.subplot(717)\n",
    "food_facts_data[food_facts_data.thickeners_stabilisers_emulsifiers != 0].product_name.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top products with thickeners_stabilisers_emulsifiers additives\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing brand dependencies of additives\n",
    "plt.figure(figsize=(5,35))\n",
    "plt.subplot(711)\n",
    "food_facts_data[food_facts_data.preservatives != 0].brands.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top companies producing foods with preservatives\")\n",
    "plt.subplot(712)\n",
    "food_facts_data[food_facts_data.acidity_regulators != 0].brands.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top companies producing foods with acidity_regulator additives\")\n",
    "plt.subplot(713)\n",
    "food_facts_data[food_facts_data.anticaking_agents != 0].brands.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top companies producing foods with anticaking_agents additives\")\n",
    "plt.subplot(714)\n",
    "food_facts_data[food_facts_data.color != 0].brands.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top companies producing foods with color additives\")\n",
    "plt.subplot(715)\n",
    "food_facts_data[food_facts_data.flavour_enhancers != 0].brands.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top companies producing foods with flavour_enhancers additives\")\n",
    "plt.subplot(716)\n",
    "food_facts_data[food_facts_data.glazingagents_gases_sweeteners != 0].brands.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top companies producing foods with glazingagents_gases_sweeteners additives\")\n",
    "plt.subplot(717)\n",
    "food_facts_data[food_facts_data.thickeners_stabilisers_emulsifiers != 0].brands.value_counts()[:20].plot.barh()\n",
    "plt.title(\"Top companies producing foods with thickeners_stabilisers_emulsifiers additives\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additive_types = common_additives.columns.tolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlation matrix of data\n",
    "corr_ = corr_ = food_facts_data[['acidity_regulators', 'anticaking_agents', 'color', 'flavour_enhancers',\n",
    "       'glazingagents_gases_sweeteners', 'missing', 'preservatives',\n",
    "       'thickeners_stabilisers_emulsifiers']].corr().abs()\n",
    "fig, axis = plt.subplots(figsize=(10, 8))\n",
    "sn.heatmap(corr_, mask=np.zeros_like(corr_, dtype=np.bool), cmap=sn.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Categories_en` use that to predict allergens\n",
    "\n",
    "So, below what we are trying to predict allergens of product based on categories. This is done using Random Forest since it is an algorithm which can fits the data well; we are also doing test split on 75-25% of the data. The features and labels for each of the product are boolean vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the food_facts dataset which we stored after MS2\n",
    "food_facts = pd.read_csv('data/food_facts.csv', low_memory=False)\n",
    "# Printing out categories which are null\n",
    "food_facts[food_facts['categories_en'].notnull()][['categories_en']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are trying to fill the `categories_en` which have missing values with the keyword `missing` and look for the unique ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts.categories_en.fillna('missing', inplace=True)\n",
    "food_facts.categories_en.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Categories_en` are split on the comma to make the all the categories present as a list. Then, we flatten for each of the product by categories by row and make every category lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts.categories_en = food_facts.categories_en.str.split(',')\n",
    "\n",
    "# flatten the additive_en feature to get the products with one additive\n",
    "food_categories_en_flattened = flattenColumn(food_facts, 'categories_en')\n",
    "food_categories_en_flattened.categories_en = food_categories_en_flattened.categories_en.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take the translations for each of the language present and make a Series for each of the translated languages by using the same index as of the flattened categories so that we can map easily with the real dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = ['fr', 'ru', 'de', 'es', 'da', 'ca', 'ar', 'cs',\n",
    "             'fi', 'gl', 'it', 'nl', 'pl', 'pt', 'ro', 'sv', 'sr', 'uk', 'zh', 'xx']\n",
    "\n",
    "def get_language_list(categories):\n",
    "    for l in LANGUAGES:\n",
    "        mask = categories.str.slice(start=0, stop=3) == l + ':'\n",
    "        with open('languages/{}.txt'.format(l)) as f:\n",
    "            translated_lang = f.read().splitlines()\n",
    "\n",
    "        yield mask, pd.Series(translated_lang, index=food_categories_en_flattened[mask].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the translated category for each category to the food flattened \n",
    "for mask, lang_series in get_language_list(food_categories_en_flattened.categories_en):\n",
    "    food_categories_en_flattened.loc[mask, 'categories_en'] = lang_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting distribution of Additives in the dataset\n",
    "food_categories_en_flattened.categories_en.value_counts()[:20].plot.barh()\n",
    "plt.title(\"The top 20 categories in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_categories_en_flattened.categories_en.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique categories {} out of {} all types'.format(food_categories_en_flattened.categories_en.nunique(), food_categories_en_flattened.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_categories_en_flattened[food_categories_en_flattened['categories_en'] != 'missing'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the food categories and only take the rows for which we have allergens not null and only take `categories_en` which are not missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = food_categories_en_flattened['allergens'].notnull()\n",
    "food_categories_en_flattened = food_categories_en_flattened[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_categories_en_flattened = food_categories_en_flattened[food_categories_en_flattened['categories_en'] != 'missing']\n",
    "food_categories_en_flattened.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the food categories remove all the other columns from it, since we are not interested in them and pivot each of the product so that we get one hot encoded features for each of the duplicate rows with different indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('cat_labels.csv'):\n",
    "    cat_labels = food_categories_en_flattened[['Unnamed: 0', 'categories_en']].pivot_table(index=['Unnamed: 0'], columns=['categories_en'], aggfunc=lambda x: True, fill_value=False)\n",
    "    # cat_labels.to_csv('cat_labels.csv')\n",
    "print('The category labels shape is: {}'.format(cat_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only food allergens where categories_en exists and only take the 7 features for the allergens\n",
    "t_food_allergens = food_allergens[food_allergens['categories_en'].notnull()].reset_index('key_0')[['Unnamed: 0', 'wheat/flour', 'milk/fruit/nut', 'powder/protein', 'acids', 'seafood', 'additives_l', 'others']].set_index('Unnamed: 0')\n",
    "t_food_allergens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the categories with the allergens on the same product index\n",
    "cat_allergens_data = cat_labels.merge(t_food_allergens, left_index=True, right_index=True)\n",
    "cat_allergens_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once we have all the data with the 8000+ categories as features and 7 allergens as labels. The agenda is to predict allergen for each product given the categories. This is a `multiple multi-class` classification problem since you can have more than 1 allergen present in a product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and test set by using `np.random.rand` which gives a uniform distribution and split it into 75% for training and 25% for testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_mask = np.random.rand(len(cat_allergens_data)) < 0.75\n",
    "train = cat_allergens_data[split_mask]\n",
    "test = cat_allergens_data[~split_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train data into features and labels to feed into the sklearn algorithm\n",
    "label_values = ['wheat/flour', 'milk/fruit/nut', 'powder/protein', 'acids', 'seafood_y', 'additives_l', 'others']\n",
    "\n",
    "train_features = train[train.columns.difference(label_values)]\n",
    "# train_features = train.iloc[:, [1021, 1348, 1436, 1482, 2192, 3187, 3616, 3619, 3688, 5071, 7231, 7717]]\n",
    "train_labels = train[label_values]\n",
    "train_labels = train_labels.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test data into features and labels to feed into the sklearn algorithm\n",
    "test_features = test[test.columns.difference(label_values)]\n",
    "# test_features = test.iloc[:, [1021, 1348, 1436, 1482, 2192, 3187, 3616, 3619, 3688, 5071, 7231, 7717]]\n",
    "test_labels = test[label_values]\n",
    "test_labels = test_labels.astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running the Random forest classification algorithm which takes in the training set of products and subsamples them by replaces for each of the decision trees. In our case, the number of decision trees is equal to the number of estimators (which is 100). Then for each of the decision tree, a product is given which gives it the allergen and the the average of the decision of all binary trees is taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification by SKLearn\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=0)  \n",
    "classifier.fit(train_features, train_labels)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the method of `feature_importances_`, we calculated the importance of the features for the classification. These features are the most important given the structure of the decision trees assigned by the random forest and hold the most amount of importance when voting takes place for a decision between various trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(classifier.feature_importances_ > 0.01)\n",
    "train_features.iloc[:, [1021, 1348, 1436, 1482, 2192, 3187, 3616, 3619, 3688, 5071, 7231, 7717]].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the classifier to test on the test set but also run it on the training set to what accuracy are we getting for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the classifier to predict for both training and testing sets\n",
    "y_pred_train = classifier.predict(train_features)\n",
    "y_pred_test = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot calculate the accuracy score directly since it is a `multiple multi class problem`. So, we calculate the accuracy by if the labels are the same they are correct but if the labels are not the same; if the true label is 1 but predicted is 0; we count as a missing label since it the algorithm has forgotten to classify it but has classified all other labels. If the true label is 0, and the algorithm has classified 1 then it is an incorrect label since it is an allergen which is not present in the product but is classified as if it is in the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_score(pred, truth):\n",
    "    \"\"\"\n",
    "    Get the accuracy score given by the predicted labels and truth labels.\n",
    "    \"\"\"\n",
    "    correct_labels = 0\n",
    "    incorrect_labels = 0\n",
    "    missing_labels = 0\n",
    "    \n",
    "    total_len = pred.shape[0]\n",
    "    for i in range(total_len):\n",
    "        row_pred = pred[i]\n",
    "        truth_val = truth.iloc[i].tolist()\n",
    "        for i in range(len(row_pred)):\n",
    "            if row_pred[i] == truth_val[i]:\n",
    "                correct_labels += 1\n",
    "            else:\n",
    "                if truth_val[i] == 1:\n",
    "                    missing_labels += 1\n",
    "                else:\n",
    "                    incorrect_labels += 1\n",
    "    return correct_labels, incorrect_labels, missing_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of correct, incorrect and missing for training and testing data and reporting their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_correct, train_incorrect, train_missing = get_accuracy_score(y_pred_train, train_labels)\n",
    "training_accuracy = train_correct / (train_correct + train_incorrect + train_missing)\n",
    "print('Total training accuracy: {}'.format(training_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_correct, test_incorrect, test_missing = get_accuracy_score(y_pred_test, test_labels)\n",
    "testing_accuracy = test_correct / (test_correct + test_incorrect + test_missing)\n",
    "print('Total testing accuracy: {}'.format(testing_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
